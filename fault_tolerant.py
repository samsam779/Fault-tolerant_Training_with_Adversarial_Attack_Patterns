# -*- coding: utf-8 -*-
"""「「SNN_MINST.ipynb」的副本」的副本

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1yvEgy7zSlzIxmzWxc_ojYyc2cNI3Ss6O

# Import Packages
"""

import os
import sys
import math
import glob
from tqdm import tqdm
import torch
from torch import nn
import torchvision
import random
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd
from torch.utils.data import Dataset, DataLoader, random_split, ConcatDataset, TensorDataset
from scipy.stats import f
import time

"""# Utility"""

def same_seed(seed): 
    '''Fixes random number generator seeds for reproducibility.'''
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False
    np.random.seed(seed)
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed_all(seed)

def view_weight(model, normalize=False):
    '''Visulize weights of the model'''
    n_param = len(model.layers)
    fig, axes = plt.subplots(n_param, 1)    
    for i, layer in enumerate(model.layers): 
            ax = axes[i]
            ax.axis('off')
            ax.set_title(f'layer: {i}')
            data = model.layers[i].weight.data.detach().cpu()
            if (normalize):
                data -= data.min()
                data /= data.max()
                data = data * 2 - 1                
            sns.heatmap(data, ax=ax, cmap='Spectral')
    plt.show()

def view_data(data, row, col):
    """Visualize dataset"""
    if row==1 and col==1:
      plt.imshow(data.to('cpu').numpy().squeeze().resize(28,28), cmap='gray')
    else:
      fig, axes = plt.subplots(row, col)
      for i in range(row):
          for j in range(col):
              ax = axes[i][j]
              sample_idx = torch.randint(len(data), size=(1,)).item()
              img, label = data[sample_idx]
              ax.axis('off')
              ax.set_title(f'{label}')
              ax.imshow(img.squeeze(), cmap='gray')
      plt.show()

def view_data_compare(data1, data2, row):
    """Visualize dataset"""
    if row==1 :
      plt.imshow(data.to('cpu').numpy().squeeze().resize(28,28), cmap='gray')
    else:
      fig, axes = plt.subplots(row, 2)
      for i in range(row):
          sample_idx = torch.randint(len(data1), size=(1,)).item()
          for j in range(2):
              if j == 0:
                ax = axes[i][j]
                # sample_idx = torch.randint(len(data1), size=(1,)).item()
                img, label = data1[sample_idx]
                ax.axis('off')
                ax.set_title(f'{label}')
                ax.imshow(img.squeeze(), cmap='gray')
              else:
                ax = axes[i][j]
                #sample_idx = torch.randint(len(data2), size=(1,)).item()
                img, label = data2[sample_idx]
                ax.axis('off')
                ax.set_title(f'{label}')
                ax.imshow(img.squeeze(), cmap='gray')
      plt.show()

def train_valid_split(data_set, valid_ratio, seed):
    '''Split provided training data into training set and validation set'''
    valid_set_size = int(valid_ratio * len(data_set)) 
    train_set_size = len(data_set) - valid_set_size
    train_set, valid_set = random_split(data_set, [train_set_size, valid_set_size], generator=torch.Generator().manual_seed(seed))
    return train_set, valid_set

def predict(test_loader, model, device):    
    model.eval() # Set your model to evaluation mode.
    total_correct = []
    preds = []
    for x, y in test_loader:
        x, y = x.to(device), y.to(device)
        with torch.no_grad():       
            logits = model(x)
            pred = logits.argmax(dim=-1).to(device)
            correct = (pred == y).float().mean()
        preds.append(pred.detach().cpu())
        total_correct.append(correct)
    pred_acc = sum(total_correct) / len(total_correct)    
    print(f"[ Predict ] acc = {pred_acc:.5f}")        
    preds = torch.cat(preds, dim=0).numpy()  
    return preds, pred_acc.item()

def save_pred(preds, file):
    ''' Save prediction '''
    df = pd.DataFrame()
    df["Id"] = list(range(len(preds)))
    df["Category"] = preds
    df.to_csv(file, index=False)

"""## Dataset"""

class ATCPGDataset(Dataset):
    '''    
    Dataset for ATCPG
    x: Images
    '''
    def __init__(self, x):        
        self.x = x

    def __getitem__(self, idx):
        return self.x[idx]

    def __len__(self):
        return len(self.x)

"""# SNN




## Model
"""

# # Parameters of SNN model
# time_window_num = 25
time_window_num = 10
thresh = 0.5 # fire threshold
lens = 0.5 # hyper-parameters of approximate function, orig:0.5
decay = 0.2 # decay constants

class SCNN(nn.Module):
    def __init__(self, shape, device):
        """
        Args:        
            shape: shape for the layers, e.g. shape = [784, 128, 10] results in layers of 784x128 and 128x10
        Members:
            device: device on which the model is          
            shape: shape of layers            
            layers: all layers of the model
            spike_factors: intensity of spikes
            mem_offsets: initial bias of membrane potential            
        """
        super(SCNN, self).__init__()
        self.device = device        
        self.shape = shape        
        
        self.layers = nn.ModuleList()
        
        for i in range(len(shape)-1):
            self.layers.append(nn.Linear(shape[i], shape[i+1], bias=False))                        
        self.layers = nn.Sequential(*self.layers)

        self.spike_factors = []
        for i in range(len(shape)):
            self.spike_factors.append(torch.ones(shape[i], device=device))

        self.mem_offsets = []
        for i in range(len(shape)):
            self.mem_offsets.append(torch.zeros(self.shape[i], device=device))        

    def forward(self, input, time_window=time_window_num, init_membranes=None, init_spikes=None, require_grad = False):        
        input = input.to(self.device)
        membranes = []
        spikes = []        
        sum_spike_output = torch.zeros(input.shape[0], self.shape[-1], device=self.device)
        spike_factors = []
        mem_offsets = []

        # Initialize membranes & spikes        
        for i in range(len(self.shape)):
            spike_factors.append(self.spike_factors[i].repeat(input.shape[0], 1))
            mem_offsets.append(self.mem_offsets[i].repeat(input.shape[0], 1))
            
        if init_membranes is None:        
            for i in range(len(self.shape)):
                spikes.append(torch.zeros(input.shape[0], self.shape[i], device=self.device))
                membranes.append(torch.zeros(input.shape[0], self.shape[i], device=self.device))
        else:            
            spikes = init_spikes
            membranes = init_membranes


        for step in range(time_window): # simulation time steps       
            if(require_grad == False):
                spikes[0] = torch.clamp(input.view(input.shape[0], -1) * spike_factors[0] + mem_offsets[0], 0, 1) # insert faults at input neurons if any            
                spikes[0] = torch.bernoulli(spikes[0]) # prob. firing  
            else:
                spikes[0] = torch.clamp(input.view(input.shape[0], -1) * spike_factors[0] + mem_offsets[0], 0, 1).float()          
            # spikes[0] = torch.bernoulli(input.view(input.shape[0], -1)) # prob. firing
            # spikes[0] = torch.clamp(spikes[0] * spike_factors[0] + mem_offsets[0], 0, 1) # insert faults at input neurons if any
                            
            # The other layers            
            for i in range(len(self.layers)):                
                membranes[i+1], spikes[i+1] = mem_update(self.layers[i], spikes[i], membranes[i+1], spikes[i+1], spike_factors[i+1], mem_offsets[i+1])            
            sum_spike_output += spikes[-1]               
            # print(sum(spikes[0]))
            # print(spikes[1])
            # print(membranes[1])

        # outputs = sum_spike_output / time_window
        outputs = sum_spike_output
        
        return outputs
# class SCNN(nn.Module):
#     def __init__(self, shape, device):
#         """
#         Args:        
#             shape: shape for the layers, e.g. shape = [784, 128, 10] results in layers of 784x128 and 128x10
#         Members:            
#             layers: all layers of the model
#             shape: shape of layers
#             spike_factors: intensity of spikes
#         """
#         super(SCNN, self).__init__()
#         self.device = device
#         self.shape = shape
        
#         self.layers = nn.ModuleList()
#         for i in range(len(shape)-1):
#             self.layers.append(nn.Linear(shape[i], shape[i+1]))        
#         self.layers = nn.Sequential(*self.layers)

#         self.spike_factors = []
#         for i in range(len(shape)):
#             self.spike_factors.append(torch.ones(shape[i], device=device))

#         self.mem_offsets = []
#         for i in range(len(shape)):
#             self.mem_offsets.append(torch.zeros(self.shape[i], device=device))                            

#     def forward(self, input, time_window=time_window_num):   
#     # def forward(self, input, time_window=time_window_num, init_membranes=None, init_spikes=None):     
#         membranes = []
#         spikes = []
#         sum_spike_output = torch.zeros(input.shape[0], self.shape[-1], device=self.device)
#         spike_factors = []
#         mem_offsets = []

#         # Initialize membranes & spikes        
#         for i in range(len(self.shape)):
#             spike_factors.append(self.spike_factors[i].repeat(input.shape[0], 1))
#             mem_offsets.append(self.mem_offsets[i].repeat(input.shape[0], 1))
#             spikes.append(torch.zeros(input.shape[0], self.shape[i], device=self.device))
#             membranes.append(torch.zeros(input.shape[0], self.shape[i], device=self.device))            

#         # if init_membranes is None:        
#         #     for i in range(len(self.shape)):
#         #         spikes.append(torch.zeros(input.shape[0], self.shape[i], device=self.device))
#         #         membranes.append(torch.zeros(input.shape[0], self.shape[i], device=self.device))
#         # else:            
#         #     spikes = init_spikes
#         #     membranes = init_membranes
        
#         for step in range(time_window): # simulation time steps                                    
#             # spikes[0] = torch.clamp(input.view(input.shape[0], -1) * torch.clamp(spike_factors[0], 0, 10) + torch.clamp(-spike_factors[0], 0, 10), 0, 1) # insert faults at input neurons if any   
#             spikes[0] = torch.clamp(input.view(input.shape[0], -1) * spike_factors[0] + mem_offsets[0], 0, 1) # insert faults at input neurons if any   
#             spikes[0] = torch.bernoulli(spikes[0]) # prob. firing    
                                    
#             # The other layers            
#             for i in range(len(self.layers)):                
#                 membranes[i+1], spikes[i+1] = mem_update(self.layers[i], spikes[i], membranes[i+1], spikes[i+1], spike_factors[i+1], mem_offsets[i+1])
#             sum_spike_output += spikes[-1]
                
#         outputs = sum_spike_output / time_window
#         return outputs

class ActFun(torch.autograd.Function):
    """ define approximate firing function """
    @staticmethod
    def forward(ctx, input):
       ctx.save_for_backward(input)
       return input.gt(thresh).float()

    @staticmethod
    def backward(ctx, grad_output, retain_graph=True):
       input, = ctx.saved_tensors
       grad_input = grad_output.clone()
       temp = abs(input - thresh) < lens
       return grad_input * temp.float()

act_fun = ActFun.apply

def mem_update(ops, x, mem, spike, factors, offset):
    """
    Update membrane potential
    """	
    spike_in = ops(x)    
    # mem = mem * decay * (1. - spike) + spike_in * torch.clamp(factors, 0, 10) + torch.clamp(-factors, 0, 10)
    mem = mem * decay * (1. - spike) + spike_in * factors + offset

    spike = act_fun(mem) # act_fun : approximation firing function
    return mem, spike

"""## Fault Injection Method"""

fault_factor = 10 # fault strength

factor_dict = {    
    'ESF': fault_factor, # positive value for spike tendency factor
    'HSF': 1 / fault_factor,
    'NASF': -10 * thresh # negative value for always spike cumulation
}

def insert_neuron_fault(model, fault_list):
    """
    Generate neuron fault behaviour
    Args:
        model: SNN model
        fault_list: [(layer_id, neuron_id), fault]
    """    
    for (layer_id, neuron_id), val in fault_list:
        # fault_list[i] = ((layer_id, neuron_id), model.spike_factors[layer_id][neuron_id].item())
        model.spike_factors[layer_id][neuron_id] = max(val, 0)
        model.mem_offsets[layer_id][neuron_id] = max(-val, 0)

def insert_synapse_fault(model, fault_list):
    """
    Generate synapse fault behaviour
    !! Note content of fault_list will be modified to the original weight data if it is SWF!!
    Args:
        model: model to be inserted faults
        fault_list: [((source_layer_id, source_neuron_id), (target_layer_id, target_neuron_id))]
    """
    for i, ((src_lid, src_nid), (tgt_lid, tgt_nid), val) in enumerate(fault_list):
        if val == np.inf: # SASF            
            model.mem_offsets[tgt_lid][tgt_nid] = model.layers[src_lid].weight.data[tgt_nid][src_nid].item() if model.mem_offsets[tgt_lid][tgt_nid] == 0 else 0
        else: # SWF
            fault_list[i][-1] = model.layers[src_lid].weight.data[tgt_nid][src_nid].item() # Replace faulty value with original weight
            model.layers[src_lid].weight.data[tgt_nid][src_nid] = val # Replace original weight with faulty value        

def recover_neuron_fault(model, fault_list):
    """recover to fault-free state"""
    for i in range(len(model.spike_factors)):
        model.spike_factors[i] = torch.ones_like(model.spike_factors[i])
        model.mem_offsets[i] = torch.zeros_like(model.spike_factors[i])

def gen_complete_fault_list(model, fault_type):
    complete_fault_list = []
    if fault_type == 'SWF':
        for i in range(len(model.shape)-1):
            for j in range(model.shape[i]):
                for k in range(model.shape[i+1]):
                    complete_fault_list.append([(i, j), (i+1, k), 1])
    else:
        for i in range(len(model.shape) - 1):
            for j in range(model.shape[i]):
                complete_fault_list.append(((i, j), factor_dict[fault_type]))
    return complete_fault_list

"""# Generate new pattern"""

# 5/11 batch size to one, loss use cross entropy method
def gen_pattern(model, data_loader, fault_list, device):
    '''Test pattern generation'''
    #epsilon: the strength of the perturbation
    epsilon = config['epsilon']
    crossentropyloss=nn.CrossEntropyLoss()
    x_adv_ret, p_val_ret = None, 1
    model.to(device)
    model.eval()
    pattern_number_generated = 0
    batch_number = 0

    # for x in data_loader:   
    for x,y in data_loader:   
        
        if pattern_number_generated >= 100:
            break

        ff_membranes, ft_membranes, ff_spikes, ft_spikes = [], [], [], []
        for i in range(len(model.shape)): #3 64 784
            ff_membranes.append(torch.zeros(x.shape[0], model.shape[i], device=device))
            ft_membranes.append(torch.zeros(x.shape[0], model.shape[i], device=device))
            ff_spikes.append(torch.zeros(x.shape[0], model.shape[i], device=device))
            ft_spikes.append(torch.zeros(x.shape[0], model.shape[i], device=device))
            # print(ff_membranes[i])
        
        x = x.to(device)
        y = y.to(device)
        x = x.view(config['batch_size'], 784)
        
        x_adv = x.detach().clone() # initialize x_adv as original benign image x        
        x_adv = x_adv.to(device)
        x_origin = x.detach().clone() # initialize x_adv as original benign image x        
        x_origin = x_adv.to(device)
        #print("x", x)
        # print("x_adv", x_adv)
        

        
        
        result_mark = torch.zeros([config['batch_size']], device=device)
        for time in range(20):     
                   
            ff_inputs = torch.bernoulli(x_adv) # encoding
            ft_inputs = torch.bernoulli(x_adv)
            
            ff_inputs.requires_grad_() # set required grad to obtain grad for attack
            ft_inputs.requires_grad_()
            
            
            ff_outputs = good_inference(model, ff_inputs, 1, 1, ff_membranes, ff_spikes,True)            
            ff_membranes = [ff_membrane.detach() for ff_membrane in ff_membranes]    
            ff_spikes = [ff_spikes.detach() for ff_spikes in ff_spikes]   
            # ff_outputs = good_inference(model, ff_inputs, 1, 1)
            model.zero_grad() # reset model grad to zero   
                       

            ft_outputs = bad_inference(model, ft_inputs, fault_list, 1, 1, ft_membranes, ft_spikes,True)
            ft_membranes = [ft_membrane.detach() for ft_membrane in ft_membranes]    
            ft_spikes = [ft_spikes.detach() for ft_spikes in ft_spikes]    
            # ft_outputs = bad_inference(model, ft_inputs, fault_list, 1, 1)
            model.zero_grad()
            

            

            # loss = torch.sum((ff_outputs - ft_outputs).pow(2)) # calculate loss
            # loss = torch.sum(( ff_outputs - label.cpu() ).pow(2) - (ff_outputs - label.cpu() ).pow(2)) # calculate loss
            # loss = torch.sum(( ff_outputs - label ).pow(2) - (ft_outputs - label ).pow(2)) # calculate loss
            # loss = criterion(ff_outputs, ft_outputs)
            #print("ft_outputs:",ft_outputs)
            ff_outputs = torch.mean(ff_outputs,dim = 1) #dim:64
            ft_outputs = torch.mean(ft_outputs,dim = 1)
            #print("ft_outputs:",ft_outputs)
            

            
            # loss = crossentropyloss(ff_outputs,y) - crossentropyloss(ft_outputs,y)
            loss = torch.sum((ff_outputs - ft_outputs).pow(2))
            # print("loss: ",loss)
            loss.backward() # calculate gradient
            # print("ff_inputs.grad",ff_inputs.grad)
            ff_grad = ff_inputs.grad.detach().clone()
            ft_grad = ft_inputs.grad.detach().clone()
            
            
            
            with torch.no_grad():
                different_time = torch.zeros(config['batch_size'], device=device)
                for checktime in range(5):   
                    ff_outputs, ft_outputs = both_inference(model, x_adv, fault_list, config['repetition'])
                    # ff_outputs_ = good_inference(model, x_adv, config['repetition'])
                    ff_max_output = torch.zeros([config['batch_size'], config['repetition'], 1], device=device)
                    ft_max_output = torch.zeros([config['batch_size'], config['repetition'], 1], device=device)
                    
                    ff_outputs = torch.mean(ff_outputs,1)
                    ft_outputs = torch.mean(ft_outputs,1)
                    
                    ff_max_output = torch.argmax(ff_outputs,1)#64
                    ft_max_output = torch.argmax(ft_outputs,1)
                    for i in range(config['batch_size']):
                    #   if time == 0:
                    #     print("ff_max_output[",i,"]:",ff_max_output[i],"   ft_max_output[",i,"]:",ft_max_output[i], "    y[",i,"]:", y[i])
                        if (ff_max_output[i].item() != ft_max_output[i].item()) & (ff_max_output[i].item() == y[i].item()) & ((result_mark[i] == 0)):
                            #x_adv_copy = torch.from_numpy(x_adv[i])
                            different_time[i] += 1
                    for i in range(config['batch_size']):
                        if (different_time[i] >=4) & (result_mark[i]==0):
                            if time == 0:
                                result_mark[i] = 1
                                continue
                            if x_adv_ret == None:
                                x_adv_ret = x_adv[i].view(1,1,28,28)
                                lab_ret = y[i].view(1,1)
                                x_origin_ret = x_origin[i].view(1,1,28,28)
                                result_mark[i] = 1
                                pattern_number_generated = pattern_number_generated+1
                                # print("find")
                            else:
                                x_adv_resize = x_adv[i].view(1,1,28,28)
                                lab_resize = y[i].view(1,1)
                                x_origin_resize = x_origin[i].view(1,1,28,28)
                                x_adv_ret = torch.cat((x_adv_ret,x_adv_resize),0)
                                lab_ret = torch.cat((lab_ret,lab_resize),0)
                                x_origin_ret = torch.cat((x_origin_ret,x_origin_resize),0)
                                result_mark[i] = 1
                                pattern_number_generated = pattern_number_generated+1
                                # print("find")
    
            # ---------- Adversarial attack ----------
            # fgsm: use gradient ascent on x_adv to maximize loss            
            x_adv_tmp = x_adv
            # print(x_adv_tmp)
            x_adv = x_adv + epsilon * (ff_grad - ft_grad).sign() 
            # print(x_adv)
            # x_adv = x_adv + epsilon * ft_grad.sign()
            # x_adv = x_adv + epsilon * (ff_grad - ft_grad)
            x_adv = torch.clamp(x_adv, 0, 1)
            
            ff_inputs.grad.zero_() # reset input grad to zero
            ft_inputs.grad.zero_()
        
        print("#batch:",batch_number," accumulate ", pattern_number_generated, " patterns")
        batch_number += 1
        
            
    
    return x_adv_ret, lab_ret, x_origin_ret
#%% ---------- Inference ---------- %%#
def good_inference(model, inputs, times=1, time_window=time_window_num, init_membranes=None, init_spikes=None, require_grad = False):
    '''
    Inference for fault-free scenario
    Args:
        times: number of repetition
        time_window: number of frames
        init_membranes: initial membranes
        init_spikes: initial spikes
    '''    
    x = inputs.repeat(times, 1)                                         # Extend inputs for parallel inference
    outputs = model(x, time_window, init_membranes, init_spikes,require_grad)        # dim.: (repetit, batch, # of out)        
    return outputs.view(times, inputs.shape[0], -1).permute(1, 0, 2)    # dim.: (batch, repetit, # of out)

def bad_inference(model, inputs, fault_list, times=1, time_window=time_window_num, init_membranes=None, init_spikes=None, require_grad = False):
    '''
    Inference for faulty scenario
    Args:
        times: number of repetition
        time_window: number of frames
        init_membranes: initial membranes
        init_spikes: initial spikes
    '''    
    x = inputs.repeat(times, 1)                                         # Extend inputs for parallel inference    
    insert_fault(model, fault_list)                                     # Insert fault    
    outputs = model(x, time_window, init_membranes, init_spikes,require_grad)        # dim.: (repetit, batch, # of out)    
    recover_fault(model, fault_list)                                    # Recover fault
    
    return outputs.view(times, inputs.shape[0], -1).permute(1, 0, 2)    # dim.: (batch, repetit, # of out)

def both_inference(model, inputs, fault_list, times=1, time_window=time_window_num):
    '''Inference both for fault-free & faulty scenario'''
    x = inputs.repeat(times, 1)
    # print('ff')    
    ff_outputs = good_inference(model, x, 1, time_window).view(times, inputs.shape[0], -1).permute(1, 0, 2)
    # print('ft')
    ft_outputs = bad_inference(model, x, fault_list, 1, time_window).view(times, inputs.shape[0], -1).permute(1, 0, 2)    
    
    return ff_outputs, ft_outputs
#%%
#%% ---------- Statistical Test ---------- %%#
def t2test(ff_outputs, ft_outputs):
    '''Hotelling's T-square test'''
    ff_outputs, ft_outputs = ff_outputs.cpu(), ft_outputs.cpu()    
    p_vals = torch.zeros(ff_outputs.shape[0])
    for i in range(ff_outputs.shape[0]):
        # p_vals[i] = multivariate_ttest(ff_outputs[i], ft_outputs[i], paired=True).loc['hotelling', 'pval']    
        p_vals[i] = multivariate_ttest_ATCPG(ff_outputs[i], ft_outputs[i], paired=True)    
    
    return p_vals
def multivariate_ttest_ATCPG(X, Y=None, paired=False):    
    '''Accelerated multivariate test for ATCPG'''
    x = np.asarray(X)    
    
    y = np.asarray(Y)    

    # Shape of arrays
    nx, k = x.shape    
    n = nx
    
    # Paired two sample
    d = x - y
    cov = np.cov(x - y, rowvar=False)        
    diff = x.mean(0) - y.mean(0)
    inv_cov = np.linalg.pinv(cov, hermitian=True)    
    t2 = (diff @ inv_cov) @ diff * n
    
    # F-value, degrees of freedom and p-value
    fval = t2 * (n - k) / (k * (n - 1))    
    df1 = k
    df2 = n - k
    pval = 1 if fval == 0 else f.sf(fval, df1, df2)    

    return pval
#%%

"""# Configuration"""

device = 'cuda' if torch.cuda.is_available() else 'cpu'

fault_type = 'HSF'
SL = 0.0001

config = {
    'seed': 777,
    # 'pretrain': False,
    'pretrain': True,
    'learning_rate': 0.001,
    'n_epoch': 20,
    #'batch_size': 1,#64
    'valid_ratio': 0.2,    
    'weight_decay': 0,
    'save_path': f'./models/model_faultfree_{fault_type}.ckpt',
    
    # --- For fault-tolerant training ---
    'train_fault_percent': 0.05, 
    'test_fault_percent': 0.2,
    'fault_save_path': f'./models/model_{fault_type}.ckpt',
    'fault_save_path_our_method': f'./models/model_our_mothod_{fault_type}.ckpt',
    'fault_save_path_our_method_again': f'./models/model_our_mothod_{fault_type}_again.ckpt',

    # atcpg
    # 'seed': 777,
    # 'fault_type': 'NASF', change 'fault_type' to fault_type
    'SL': 0.0001,
    # change learning rate. n_epoch name for pattern generation 
    # 'learning_rate': 1e-4, 
    # 'n_epoch': 600,
    'n_samples': 256,
    'batch_size': 64, #64     # batch_size must be the factor of n_samples
    # 'repetition': 100,#100
    'repetition': 20,
    'efficacy': 0.5,
    'epsilon': 0.1,
    'rram': False,
    # 'opt': '_manual',   # optional
    'opt': '',   # optional,
    'wgt_stck_val': 1, # SWF weight stuck at value 
    'nrn_ft_fct': 10 # neuron fault spike factor

}

# fc layer
layers = [28*28, 128, 10]
# layers = [28*28, 256, 32, 10]

"""# Fault-tolerant training

## Fault Injection
"""

# ---------- Set fault injection function ----------
if fault_type == 'SWF':
    insert_fault = insert_synapse_fault
    recover_fault = insert_synapse_fault
else:
    insert_fault = insert_neuron_fault
    recover_fault = recover_neuron_fault

"""## Trainer"""

def trainer(train_loader, valid_loader, model, config, device):
    '''Training without consider fault effect'''
    if not os.path.isdir('./models'):
        os.mkdir('./models') # Create directory of saving models.
    
    n_epoch, lr = config['n_epoch'], config['learning_rate']
    criterion = nn.CrossEntropyLoss()
    optimizer = torch.optim.Adam(model.parameters(), lr=config['learning_rate'])    
    lr_lambda = lambda epoch: lr * (0.9 ** epoch)
    # scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lr_lambda)
    scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=1)  
          
    best_loss, best_acc, step = math.inf, 0, 0    
    # ---------- Training ----------
    model.train() # Set your model to train mode.
    for epoch in range(n_epoch):    
        # These are used to record information in training.
        train_loss = []
        train_acc = []        
        
        # tqdm is a package to visualize your training progress.
        train_pbar = tqdm(train_loader, position=0, leave=True) 

        for x, y in train_pbar:
            optimizer.zero_grad()               # Set gradient to zero.
            x, y = x.to(device), y.to(device)   # Move your data to device. 
            
            logits = model(x)                        

            L1_norm = sum(p.abs().sum() for p in model.parameters()) # L1 regularization
            loss = criterion(logits, y) + config['weight_decay'] * L1_norm

            loss.backward()                     # Compute gradient(backpropagation).
            optimizer.step()                    # Update parameters.                        
            step += 1
            
            train_loss.append(loss.detach().item())
            
            # Compute the accuracy for current batch.
            acc = (logits.argmax(dim=-1) == y.to(device)).float().mean()
            train_acc.append(acc)            

            # Display current epoch number and loss on tqdm progress bar.
            train_pbar.set_description(f'Epoch [{epoch+1}/{n_epoch}]')
            train_pbar.set_postfix({'loss': loss.detach().item(), 'acc': acc.detach().item()})
        
        mean_train_loss = sum(train_loss)/len(train_loss)
        mean_train_acc = sum(train_acc)/len(train_acc)
        
        scheduler.step() # Update learning rate scheduler

        # ---------- Validation ----------
        model.eval() # Set your model to evaluation mode.
        valid_loss = []
        valid_acc = []
        for x, y in valid_loader:
            x, y = x.to(device), y.to(device)
            with torch.no_grad():
                logits = model(x)
                loss = criterion(logits, y)
            valid_loss.append(loss.item())
            acc = (logits.argmax(dim=-1) == y).float().mean()        
            valid_acc.append(acc)

        mean_valid_loss = sum(valid_loss) / len(valid_loss)
        mean_valid_acc = sum(valid_acc) / len(valid_acc)        
        print(f"[ Valid | {epoch + 1:03d}/{n_epoch:03d} ] loss = {mean_valid_loss:.5f}, acc = {mean_valid_acc:.5f}")        

        if mean_valid_acc > best_acc: # Achieve higher acc than record
            best_acc = mean_valid_acc
            torch.save(model.state_dict(), config['save_path']) # Save your best model
            print('Saving model with acc {:.3f}...'.format(best_acc))
    return mean_valid_acc

"""## Fault-tolerant Trainer"""

def trainer_ft(train_loader, valid_loader, model, config, device, complete_fault_list,ourmethod):
    '''Training with fault effect considered'''
    if not os.path.isdir('./models'):
        os.mkdir('./models') # Create directory of saving models.
    
    n_epoch, lr = config['n_epoch'], config['learning_rate']
    criterion = nn.CrossEntropyLoss()
    optimizer = torch.optim.Adam(model.parameters(), lr=config['learning_rate'])    
    lr_lambda = lambda epoch: lr * (0.9 ** epoch)
    # scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lr_lambda)
    scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=1)  
            
    best_loss, best_acc, step = math.inf, 0, 0    
    # ---------- Training ----------
    model.train() # Set your model to train mode.
    for epoch in range(n_epoch):    
        # These are used to record information in training.
        train_loss = []
        train_acc = []        
        
        # tqdm is a package to visualize your training progress.
        train_pbar = tqdm(train_loader, position=0, leave=True) 

        for x, y in train_pbar:
            optimizer.zero_grad()               # Set gradient to zero.
            x, y = x.to(device), y.to(device)   # Move your data to device. 
                        
            # hsin 06/14
            fault_list = random.sample(complete_fault_list, int(len(complete_fault_list) * config['train_fault_percent']))
            insert_fault(model, fault_list)
            logits = model(x) # faulty inference
            recover_fault(model, fault_list)

            loss = criterion(logits, y)

            loss.backward()                     # Compute gradient(backpropagation).
            optimizer.step()                    # Update parameters.                        
            step += 1
            
            train_loss.append(loss.detach().item())
            
            # Compute the accuracy for current batch.
            acc = (logits.argmax(dim=-1) == y.to(device)).float().mean()
            train_acc.append(acc)            

            # Display current epoch number and loss on tqdm progress bar.
            train_pbar.set_description(f'Epoch [{epoch+1}/{n_epoch}]')
            train_pbar.set_postfix({'loss': loss.detach().item(), 'acc': acc.detach().item()})        
        
        mean_train_loss = sum(train_loss)/len(train_loss)
        mean_train_acc = sum(train_acc)/len(train_acc)        
        
        scheduler.step() # Update learning rate scheduler

        # ---------- Validation ----------
        model.eval() # Set your model to evaluation mode.
        valid_loss = []
        valid_acc = []
        for x, y in valid_loader:
            x, y = x.to(device), y.to(device)
            with torch.no_grad():
                logits = model(x)
                loss = criterion(logits, y)
            valid_loss.append(loss.item())
            acc = (logits.argmax(dim=-1) == y).float().mean()        
            valid_acc.append(acc)

        mean_valid_loss = sum(valid_loss) / len(valid_loss)
        mean_valid_acc = sum(valid_acc) / len(valid_acc)        
        print(f"[ Valid | {epoch + 1:03d}/{n_epoch:03d} ] loss = {mean_valid_loss:.5f}, acc = {mean_valid_acc:.5f}")        

        if mean_valid_acc > best_acc: # Achieve higher acc than record
            if ourmethod == 0:
                best_acc = mean_valid_acc
                torch.save(model.state_dict(), config['fault_save_path']) # Save your best model
                print('Saving model with acc {:.3f}...'.format(best_acc))
            if ourmethod == 1:
                best_acc = mean_valid_acc
                torch.save(model.state_dict(), config['fault_save_path_our_method']) # Save your best model
                print('Saving model with acc {:.3f}...'.format(best_acc))
    return mean_valid_acc

"""## Load Data"""
def trainer_ft_new(train_loader, valid_loader, model, config, device, complete_fault_list,ourmethod):
    '''Training with fault effect considered'''
    if not os.path.isdir('./models'):
        os.mkdir('./models') # Create directory of saving models.
    
    n_epoch, lr = config['n_epoch'], config['learning_rate']
    criterion = nn.CrossEntropyLoss()
    optimizer = torch.optim.Adam(model.parameters(), lr=config['learning_rate'])    
    lr_lambda = lambda epoch: lr * (0.9 ** epoch)
    # scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lr_lambda)
    scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=1)  
    data_loader = train_loader        
    concat_data = train_data
    best_loss, best_acc, step = math.inf, 0, 0    
    fault_list_accumulate = []
    # ---------- Training ----------
    model.train() # Set your model to train mode.
    for epoch in range(n_epoch):    
        # These are used to record information in training.
        train_loss = []
        train_acc = []        
        
        # tqdm is a package to visualize your training progress.
        train_pbar = tqdm(data_loader, position=0, leave=True) 

        for x, y in train_pbar:
            optimizer.zero_grad()               # Set gradient to zero.
            x, y = x.to(device), y.to(device)   # Move your data to device. 
                        
            # hsin 06/14
            fault_list = random.sample(complete_fault_list, int(len(complete_fault_list) * config['train_fault_percent']))
            fault_list_accumulate.append(fault_list)
            insert_fault(model, fault_list)
            logits = model(x) # faulty inference
            recover_fault(model, fault_list)

            loss = criterion(logits, y)

            loss.backward()                     # Compute gradient(backpropagation).
            optimizer.step()                    # Update parameters.                        
            step += 1
            
            train_loss.append(loss.detach().item())
            
            # Compute the accuracy for current batch.
            acc = (logits.argmax(dim=-1) == y.to(device)).float().mean()
            train_acc.append(acc)            

            # Display current epoch number and loss on tqdm progress bar.
            train_pbar.set_description(f'Epoch [{epoch+1}/{n_epoch}]')
            train_pbar.set_postfix({'loss': loss.detach().item(), 'acc': acc.detach().item()})        
        
        mean_train_loss = sum(train_loss)/len(train_loss)
        mean_train_acc = sum(train_acc)/len(train_acc)        
        
        scheduler.step() # Update learning rate scheduler

        # ---------- Validation ----------
        model.eval() # Set your model to evaluation mode.
        valid_loss = []
        valid_acc = []
        for x, y in valid_loader:
            x, y = x.to(device), y.to(device)
            with torch.no_grad():
                logits = model(x)
                loss = criterion(logits, y)
            valid_loss.append(loss.item())
            acc = (logits.argmax(dim=-1) == y).float().mean()        
            valid_acc.append(acc)

        mean_valid_loss = sum(valid_loss) / len(valid_loss)
        mean_valid_acc = sum(valid_acc) / len(valid_acc)        
        print(f"[ Valid | {epoch + 1:03d}/{n_epoch:03d} ] loss = {mean_valid_loss:.5f}, acc = {mean_valid_acc:.5f}")        

        if mean_valid_acc > best_acc: # Achieve higher acc than record
            if ourmethod == 0:
                best_acc = mean_valid_acc
                torch.save(model.state_dict(), config['fault_save_path']) # Save your best model
                print('Saving model with acc {:.3f}...'.format(best_acc))
            if ourmethod == 1:
                best_acc = mean_valid_acc
                torch.save(model.state_dict(), config['fault_save_path_our_method']) # Save your best model
                print('Saving model with acc {:.3f}...'.format(best_acc))
        if(epoch == (n_epoch - 1)): break
        img,label ,origin= gen_pattern(model, train_loader, fault_list, device)
    
        img = img.cpu()
        label = label.cpu()
        origin = origin.cpu()
        new_dataset = patternGenDataset(root='data', img = img, lab = label)

        # new_dataset = TensorDataset(img,label)
        origin_dataset = patternGenDataset(root='data', img = origin, lab = label)
        concat_data = ConcatDataset([concat_data, new_dataset])

        # view_data(new_dataset, 2, 2)
        # view_data_compare(origin_dataset,new_dataset,3)
        data_loader = DataLoader(concat_data, batch_size=config['batch_size'], shuffle=True, pin_memory=True)
    
    return mean_valid_acc, data_loader, fault_list_accumulate


def trainer_ft_new_again(train_loader, valid_loader, test_loader, model, config, device, complete_fault_list,fault_list_accumulate_test,acc_origin,ourmethod):
    '''Training with fault effect considered'''
    if not os.path.isdir('./models'):
        os.mkdir('./models') # Create directory of saving models.
    
    n_epoch, lr = config['n_epoch'], config['learning_rate']
    criterion = nn.CrossEntropyLoss()
    optimizer = torch.optim.Adam(model.parameters(), lr=config['learning_rate'])    
    lr_lambda = lambda epoch: lr * (0.9 ** epoch)
    # scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lr_lambda)
    scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=1)  
    data_loader = train_loader        
    concat_data = train_data
    best_loss, best_acc, step = math.inf, 0, 0    
    index=0
    accumulate_time_for_test = 0
    # ---------- Training ----------
    model.train() # Set your model to train mode.
    for epoch in range(n_epoch):    
        # These are used to record information in training.
        train_loss = []
        train_acc = []        
        
        # tqdm is a package to visualize your training progress.
        train_pbar = tqdm(data_loader, position=0, leave=True) 

        for x, y in train_pbar:
            optimizer.zero_grad()               # Set gradient to zero.
            x, y = x.to(device), y.to(device)   # Move your data to device. 
                        
            # hsin 06/14
            # fault_list = fault_list_accumulate[index]
            fault_list = random.sample(complete_fault_list, int(len(complete_fault_list) * config['train_fault_percent']))
            
            insert_fault(model, fault_list)
            logits = model(x) # faulty inference
            recover_fault(model, fault_list)

            loss = criterion(logits, y)

            loss.backward()                     # Compute gradient(backpropagation).
            optimizer.step()                    # Update parameters.                        
            step += 1
            
            train_loss.append(loss.detach().item())
            
            # Compute the accuracy for current batch.
            acc = (logits.argmax(dim=-1) == y.to(device)).float().mean()
            train_acc.append(acc)            

            # Display current epoch number and loss on tqdm progress bar.
            train_pbar.set_description(f'Epoch [{epoch+1}/{n_epoch}]')
            train_pbar.set_postfix({'loss': loss.detach().item(), 'acc': acc.detach().item()})        
        
        mean_train_loss = sum(train_loss)/len(train_loss)
        mean_train_acc = sum(train_acc)/len(train_acc)        
        
        scheduler.step() # Update learning rate scheduler

        # ---------- Validation ----------
        model.eval() # Set your model to evaluation mode.
        valid_loss = []
        valid_acc = []
        for x, y in valid_loader:
            x, y = x.to(device), y.to(device)
            with torch.no_grad():
                logits = model(x)
                loss = criterion(logits, y)
            valid_loss.append(loss.item())
            acc = (logits.argmax(dim=-1) == y).float().mean()        
            valid_acc.append(acc)

        mean_valid_loss = sum(valid_loss) / len(valid_loss)
        mean_valid_acc = sum(valid_acc) / len(valid_acc)        
        print(f"[ Valid | {epoch + 1:03d}/{n_epoch:03d} ] loss = {mean_valid_loss:.5f}, acc = {mean_valid_acc:.5f}")        

        if mean_valid_acc > best_acc: # Achieve higher acc than record
            # if ourmethod == 0:
            #     best_acc = mean_valid_acc
            #     torch.save(model.state_dict(), config['fault_save_path']) # Save your best model
            #     print('Saving model with acc {:.3f}...'.format(best_acc))
            # if ourmethod == 1:
            #     best_acc = mean_valid_acc
            #     torch.save(model.state_dict(), config['fault_save_path_our_method']) # Save your best model
            #     print('Saving model with acc {:.3f}...'.format(best_acc))
            best_acc = mean_valid_acc
            torch.save(model.state_dict(), config['fault_save_path_our_method_again']) # Save your best model
            print('Saving model with acc {:.3f}...'.format(best_acc))

        acc_accumulate = []
        
        start = time.process_time()
        for m in range(10):      
            # ---------- Prediction ----------    
            fault_list = fault_list_accumulate_test[m]
            insert_fault(model, fault_list)             
            preds, acc = predict(test_loader, model, device)
            recover_fault(model, fault_list)
            fault_list_accumulate.append(fault_list)
            acc_accumulate.append(acc)
        avg_acc = sum(acc_accumulate)/len(acc_accumulate)
        print('test acc: ',avg_acc)
        end = time.process_time()
        accumulate_time_for_test = accumulate_time_for_test + (end-start)
        if(avg_acc>=acc_origin):
            print('epoch: ',epoch)
            break 

        # if(epoch == (n_epoch - 1)): break
        # img,label ,origin= gen_pattern(model_origin, train_loader, fault_list, device)
    
        # img = img.cpu()
        # label = label.cpu()
        # origin = origin.cpu()
        # new_dataset = patternGenDataset(root='data', img = img, lab = label)

        # # new_dataset = TensorDataset(img,label)
        # origin_dataset = patternGenDataset(root='data', img = origin, lab = label)
        # concat_data = ConcatDataset([concat_data, new_dataset])

        # # view_data(new_dataset, 2, 2)
        # # view_data_compare(origin_dataset,new_dataset,3)
        # data_loader = DataLoader(concat_data, batch_size=config['batch_size'], shuffle=True, pin_memory=True)
    
    return mean_valid_acc,accumulate_time_for_test
"""## Load Data"""

"""## Dataset"""

class patternGenDataset(Dataset):
    '''    
    Dataset for pattern generation
    x: Images
    '''
    def __init__(self,root, img, lab, transform = None, target_transform = None):
        imgs = []
        for i in range(len(lab)):
            imgs.append((img[i],int(lab[i])))        
        # self.x = x
        self.imgs = imgs
        self.transform = transform
        self.target_transform = target_transform

    def __getitem__(self, idx):
        fn, label = self.imgs[idx]
        
        return self.imgs[idx]

    def __len__(self):
        return len(self.imgs)
    



train_data = torchvision.datasets.MNIST(root='data', train=True, download=True, transform=torchvision.transforms.ToTensor())
test_data = torchvision.datasets.MNIST(root='data', train=False, download=True, transform=torchvision.transforms.ToTensor())
train_data, valid_data = train_valid_split(train_data, config['valid_ratio'], config['seed'])


train_loader = DataLoader(train_data, batch_size=config['batch_size'], shuffle=True, pin_memory=True)
valid_loader = DataLoader(valid_data, batch_size=config['batch_size'], shuffle=True, pin_memory=True)
test_loader = DataLoader(test_data, batch_size=config['batch_size'], shuffle=False, pin_memory=True)
# gen_pattern_loader = DataLoader(train_data, batch_size=1, shuffle=True, pin_memory=True)
# print(len(train_loader))

for x,y in train_loader:
  print('x:',x.size())
  print('y:',y.size())
  break
#view_data(test_data, 3, 4)

"""# Main"""

inputs = torch.rand(config['n_samples'], 28*28) # random initial input data 
print(inputs)
print(inputs.size())
class ATCPGDataset(Dataset):
    '''    
    Dataset for ATCPG
    x: Images
    '''
    def __init__(self, x):        
        self.x = x

    def __getitem__(self, idx):
        return self.x[idx]

    def __len__(self):
        return len(self.x)       
data = ATCPGDataset(inputs) # perpare dataset
# print(data)
# print(len(data))
data_loader = DataLoader(data, batch_size=config['batch_size'], shuffle=False, pin_memory=True) # prepare dataloader
# print(data_loader)
# print('len:',len(data_loader))
# for x in data_loader:
#   print(x)
#   print(x.size())
#   break

model = SCNN(layers, device).to(device)
#model.load_state_dict(torch.load(config['fault_save_path']))
# complete_fault_list = gen_complete_fault_list(model,fault_type)
# train_loader = torch.Tensor(train_loader)
# print(train_loader)
# for x,y in train_loader:
#   print(x.size())
#   print(y.size())
#   break
# # [data_loader] = train_loader
# # print(data_loader)
# # data_loader
# gen_pattern(model, train_loader, complete_fault_list, device)
# gen_pattern(model, data_loader, complete_fault_list, device)

"""### fault-free fault-free senario"""

same_seed(config['seed'])

model = SCNN(layers, device).to(device)
model_origin = SCNN(layers, device).to(device)
base_model = SCNN(layers, device).to(device)

model_train_our = SCNN(layers, device).to(device)
complete_fault_list = gen_complete_fault_list(model,fault_type)
torch.set_printoptions(precision=3)
cycle = 8
crt_acc=0
max_acc=0
crt_acc_origin=0
max_acc_origin=0
base_acc=0
max_base_acc=0

# ---------- Training ----------
fault_list = random.sample(complete_fault_list, int(len(complete_fault_list) * config['train_fault_percent']))
train_loader_origin = train_loader
train_loader_base = train_loader
print('TRAINING BASE METHOD') 
base_acc = trainer(train_loader_base,valid_loader,base_model,config,device)
# if ( base_acc > max_base_acc ):
#   max_base_acc = base_acc
#   print('base method improving to ',max_base_acc)
# else:
#   print('base method not improving')

print('TRAINING ORIGINAL METHOD') 
start1 = time.process_time()
crt_acc_origin = trainer_ft(train_loader_origin, valid_loader, model_origin, config, device, complete_fault_list,0)
end1 = time.process_time()
print("original training time: %f"%(end1-start1))
# if ( crt_acc_origin > max_acc_origin ):
#   max_acc_origin = crt_acc_origin
#   print('original method improving to ',max_acc_origin)
# else:
#   print('original method not improving')
acc_accumulate = []
fault_list_accumulate_test = []

for _ in range(10):      
    # ---------- Prediction ----------    
    fault_list = random.sample(complete_fault_list, int(len(complete_fault_list) * config['test_fault_percent']))
    insert_fault(model_origin, fault_list)             
    preds, acc = predict(test_loader, model_origin, device)
    recover_fault(model_origin, fault_list)
    fault_list_accumulate_test.append(fault_list)
    acc_accumulate.append(acc)
avg_acc = sum(acc_accumulate)/len(acc_accumulate)
print('test acc: ',avg_acc)
acc_accumulate = []



print('TRAINING OUR METHOD') 
start2 = time.process_time()
crt_acc,new_train_loader, fault_list_accumulate = trainer_ft_new(train_loader, valid_loader, model, config, device, complete_fault_list,1)
end2 = time.process_time()
# if ( crt_acc > max_acc ):
#   max_acc = crt_acc
#   print('our method improving to ',max_acc)
# else:
#   print('our method not improving')
preds, acc_new = predict(test_loader, model, device)
print('original test acc: ',acc_new)
start3 = time.process_time()
crt_acc,accumulate_time_for_test = trainer_ft_new_again(new_train_loader, valid_loader,test_loader, model_train_our, config, device, complete_fault_list, fault_list_accumulate_test, avg_acc,1)
end3 = time.process_time()
print("original training time: %f"%(end1-start1))
print("our training time: %f"%(end2-start2))
print("our again training time: %f"%(end3-start3-accumulate_time_for_test))

    
